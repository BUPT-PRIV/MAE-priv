output_dir: 'output/pretrain/vit_giant_patch16_224_GAP'

# Architecture
model: pretrain_mae_giant_patch14_224
decoder_dim: 512
decoder_depth: 8
use_mean_pooling: True # not use cls-token
normlize_target: True
mask_ratio: 0.75

# Train
epochs: 100
opt: adamw
lr: 0.00015 # base_lr = 1.5e-4, lr = base_lr * batch-size / 256
warmup_lr: 0.000001 # 1e-6
min_lr: 0.
warmup_epochs: 10
batch_size: 512
hflip: 0.5

# Wandb
log_wandb: True
wandb_project: 'MAE-Pretrain'