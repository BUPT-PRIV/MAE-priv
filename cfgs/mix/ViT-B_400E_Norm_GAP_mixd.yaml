output_dir: 'output/pretrain/mix_mae/decoder_mix_vit_base_400e_patch16_224_GAP'

# Architecture
model: mix_mae_base_patch16_224
decoder_dim: 384
decoder_depth: 4
use_mean_pooling: True
normlize_target: True
mask_ratio: 0.75
mix_mode: 'decoder'
mix_alpha: 0.5

# Train
epochs: 400
opt: adamw
lr: 0.00015 # base_lr = 1.5e-4, lr = base_lr * batch-size / 256
warmup_lr: 0.000001 # 1e-6
min_lr: 0.
warmup_epochs: 40
batch_size: 512
hflip: 0.5

# Wandb
log_wandb: True
wandb_project: 'MAE-Pretrain'